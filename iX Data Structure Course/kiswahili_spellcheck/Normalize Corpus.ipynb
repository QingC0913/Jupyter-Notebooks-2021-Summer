{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tropical-receiver",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "import pygsheets\n",
    "import re\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ruled-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('sw_string.txt', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "electronic-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regular-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "    with open('sw_corpus_tokenized.txt', 'w+') as f:\n",
    "        for word in tokens:\n",
    "            f.write(\"%s \" % word)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "invisible-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(corpus, n):\n",
    "    \n",
    "    output=[]\n",
    "    \n",
    "    for i in range(len(corpus)-n+1):\n",
    "        output.append(' '.join(corpus[i:i+n]))\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "careful-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenizer(text)\n",
    "\n",
    "sw_one_word = {k:v for (k,v) in FreqDist(data).items()}\n",
    "sw_two_word = {k:v for (k,v) in FreqDist(ngrams(data, 2)).items()}\n",
    "sw_three_word = {k:v for (k,v) in FreqDist(ngrams(data, 3)).items()}\n",
    "sw_four_word = {k:v for (k,v) in FreqDist(ngrams(data, 4)).items()}\n",
    "\n",
    "#concatenate frequencies into one dictionary\n",
    "# ** is the dictionary unpacking operator\n",
    "sw_frequency = {**sw_one_word, **sw_two_word, **sw_three_word, **sw_four_word}\n",
    "\n",
    "sw_frequency_frame = pd.DataFrame.from_dict(sw_frequency, orient='index', columns=[\"Frequency\"])  \n",
    "sw_frequency_frame.sort_values(by=['Frequency'], ascending=False).to_csv(r'/Users/kaigiomi/Desktop/Senior Year Sem 2/Uliza/sw_frequency.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suburban-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists of easily tagged prepositions, pronouns , determiners, and some adjectives\n",
    "\n",
    "PREPOSITIONS = [\"kwa\", \"mwa\", \"wa\", \"la\", \"cha\", \"vya\", \"ya\", \"za\", \"pa\", \"na\", \"nami\", \"nawe\", \"nasi\", \"nanyi\", \"naye\", \n",
    "                \"nao\", \"nayo\", \"nalo\", \"nacho\", \"navyo\", \"nazo\", \"nako\", \"napo\", \"namu\"]\n",
    "  \n",
    "PRONOUNS = [\"mimi\", \"wewe\", \"sisi\", \"ninyi\", \"nyinyi\", \"yeye\", \"wao\"]\n",
    "\n",
    "DETERMINERS = [\"huyu\", \"hawa\", \"huu\", \"hii\", \"hilo\", \"haya\", \"hiki\", \"hivi\", \"hii\", \"hizi\", \"huku\", \"hapa\",\n",
    "               \"humu\", \"yule\", \"wale\", \"ule\", \"ile\", \"lile\", \"yale\", \"kile\", \"vile\", \"zile\", \"ule\", \"kule\", \n",
    "               \"pale\", \"mle\", \"huyo\", \"hao\", \"huo\", \"hiyo\", \"hilo\", \"hayo\", \"hicho\", \"hivyo\", \"hiyo\", \"hizo\", \n",
    "               \"huko\", \"hapo\", \"humu\"]\n",
    "\n",
    "RELATIVE_ADJECTIVES_PREFIXES = [\"ninaye\", \"niliye\", \"nitakaye\", \"nisiye\", \"unaye\", \"uliye\", \"utakaye\", \"usiye\", \n",
    "                                \"tunao\", \"tulio\", \"tutakao\", \"tusio\", \"mnao\", \"mlio\", \"mtakao\", \"msio\", \"anaye\", \n",
    "                                \"aliye\", \"atakaye\", \"asiye\", \"wanao\", \"walio\", \"watakao\", \"wasio\", \"unao\", \"ulio\", \n",
    "                                \"utakao\", \"usio\", \"inayo\", \"iliyo\", \"itakayo\", \"isiyo\", \"linalo\", \"lililo\", \n",
    "                                \"litakalo\", \"lisilo\", \"yanayo\", \"yaliyo\", \"yatakayo\", \"yasiyo\", \"kinacho\", \n",
    "                                \"kilicho\", \"kitakacho\", \"kisicho\", \"vinavyo\", \"vilivyo\", \"vitakavyo\", \"visivyo\", \n",
    "                                \"inayo\", \"iliyo\", \"itakayo\", \"isiyo\", \"zinazo\", \"zilizo\", \"zitakazo\", \"zisizo\", \n",
    "                                \"unao\", \"ulio\", \"utakao\", \"usio\", \"unao\", \"ulio\", \"utakao\", \"usio\", \"kunako\", \n",
    "                                \"kuliko\", \"kutakako\", \"kusiko\", \"panapo\", \"palipo\", \"patakapo\", \"pasipo\", \"kunako\", \n",
    "                                \"kuliko\", \"kutakako\", \"kusiko\", \"mnamo\", \"mlimo\", \"mtakamo\", \"msimo\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "introductory-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists for verb prefix combinations\n",
    "\n",
    "KNOWN_VERB_SUFFIX = [\"ana\", \"isha\", \"lisha\", \"esha\", \"lesha\"]\n",
    "\n",
    "HAVE_VERB = [\"nina\", \"sina\", \"una\", \"huna\", \"tuna\", \"hatuna\", \"mna\", \"hamna\", \"ana\", \"hana\", \"wana\", \"hawana\", \n",
    "        \"una\", \"hauna\", \"ina\", \"haina\", \"lina\", \"halina\", \"yana\", \"hayana\", \"kina\", \"hakina\", \"vina\", \n",
    "        \"havina\", \"ina\", \"haina\", \"zina\", \"hazina\", \"una\", \"hauna\", \"kuna\", \"hakuna\", \"pana\", \"hapana\",\n",
    "        \"kuna\", \"hakuna\", \"mna\", \"hamna\"]\n",
    "\n",
    "CONINUATIVE_VERB = [\"ningali\", \"ungali\", \"tungali\", \"mngali\", \"angali\", \"wangali\", \"ungali\", \"ingali\", \n",
    "                    \"lingali\", \"yangali\", \"kingali\", \"vingali\", \"ingali\", \"zingali\", \"ungali\", \"kungali\", \n",
    "                    \"pangali\", \"mngali\"]\n",
    "\n",
    "RELATIVE_VERB = [\"niliye\", \"nisiye\", \"uliye\", \"usiye\", \"tulio\", \"tusio\", \"mlio\", \"msio\", \"aliye\", \"asiye\", \n",
    "                 \"walio\", \"wasio\", \"ulio\", \"usio\", \"iliyo\", \"isiyo\", \"lililo\", \"lisilo\", \"yaliyo\", \"yasiyo\", \n",
    "                 \"kilicho\", \"kisicho\", \"vilivyo\", \"visivyo\", \"iliyo\", \"isiyo\", \"zilizo\", \"zisizo\", \"ulio\", \n",
    "                 \"usio\", \"kuliko\", \"kusiko\", \"palipo\", \"pasipo\", \"mlimo\", \"msimo\"]\n",
    "\n",
    "LOCATION_VERB = [\"nipo\", \"niko\", \"nimo\", \"sipo\", \"siko\", \"simo\", \"upo\", \"uko\", \"umo\", \"hupo\", \"huko\", \"humo\",\n",
    "                 \"tupo\", \"tuko\", \"tumo\", \"hatupo\", \"hatuko\", \"hatumo\", \"mpo\", \"mko\", \"mmo\", \"hampo\", \"hamko\", \n",
    "                 \"hammo\", \"yupo\", \"yuko\", \"yumo\", \"hayupo\", \"hayuko\", \"hayumo\", \"wapo\", \"wako\", \"wamo\", \"hawapo\", \n",
    "                 \"hawako\", \"hawamo\", \"upo\", \"uko\", \"umo\", \"haupo\", \"hauko\", \"haumo\", \"ipo\", \"iko\", \"imo\", \"haipo\", \n",
    "                 \"haiko\", \"haimo\", \"lipo\", \"liko\", \"limo\", \"halipo\", \"haliko\", \"halimo\", \"yapo\", \"yako\", \"yamo\", \n",
    "                 \"hayapo\", \"hayako\", \"hayamo\", \"kipo\", \"kiko\", \"kimo\", \"hakipo\", \"hakiko\", \"hakimo\", \"vipo\", \n",
    "                 \"viko\", \"vimo\", \"havipo\", \"haviko\", \"havimo\", \"ipo\", \"iko\", \"imo\", \"haipo\", \"haiko\", \"haimo\",\n",
    "                 \"zipo\", \"ziko\", \"zimo\", \"hazipo\", \"haziko\", \"hazimo\", \"upo\", \"uko\", \"umo\", \"haupo\", \"hauko\", \n",
    "                 \"haumo\", \"upo\", \"uko\", \"umo\", \"haupo\", \"hauko\", \"haumo\", \"kupo\", \"kuko\", \"kumo\", \"hakupo\", \n",
    "                 \"hakuko\", \"hakumo\", \"papo\", \"pako\", \"pamo\", \"hapapo\", \"hapako\", \"hapamo\", \"kupo\", \"kuko\", \n",
    "                 \"kumo\", \"hakupo\", \"hakuko\", \"hakumo\", \"mpo\", \"mko\", \"mmo\", \"hampo\", \"hamko\", \"hammo\"]\n",
    "\n",
    "COPULA_VERB = [\"ni\", \"si\", \"ndimi\", \"ndiye\", \"simi\", \"siye\", \"ndiwe\", \"ndiye\", \"siwe\", \"siye\", \"ndisi\", \"ndio\", \n",
    "               \"sio\", \"ndinyi\", \"ndio\", \"sinyi\", \"sio\", \"ndiye\", \"siye\", \"ndio\", \"sio\", \"ndiyo\", \"siyo\", \"ndilo\", \n",
    "               \"silo\", \"ndiyo\", \"siyo\", \"ndicho\", \"sicho\", \"ndivyo\", \"sivyo\", \"ndiyo\", \"siyo\", \"ndizo\", \"sizo\", \n",
    "               \"ndio\", \"sio\", \"ndio\", \"sio\", \"ndiko\", \"siko\", \"ndipo\", \"sipo\", \"ndiko\", \"siko\", \"ndimo\", \"simo\"]\n",
    "\n",
    "VERB_EQUALITY = HAVE_VERB + CONINUATIVE_VERB + RELATIVE_VERB + LOCATION_VERB + COPULA_VERB\n",
    "\n",
    "IRREALIS_VERB = [\"nge\", \"singe\", \"ngali\", \"singali\"]\n",
    "\n",
    "POS_SUBJECT_NO_A = [\"ni\", \"u\", \"tu\", \"m\", \"mw\", \"a\", \"yu\", \"wa\", \"i\", \"li\", \"ya\", \"ki\", \"vi\", \"zi\", \"ku\", \"pa\", \"mu\"]\n",
    "\n",
    "POS_SUBJECT_WITH_A = [\"n\", \"w\", \"tw\", \"mw\", \"y\", \"l\", \"ch\", \"vy\", \"z\", \"kw\", \"p\", \"mw\"]\n",
    "\n",
    "NEG_SUBJECT = [\"si\", \"hu\", \"hatu\", \"ham\", \"ha\", \"hayu\", \"hawa\", \"hau\", \"hai\", \"hali\", \"haya\", \"haki\", \"havi\", \"hazi\", \n",
    "               \"haku\", \"hapa\", \"hamu\"]\n",
    "\n",
    "POS_OBJECT = [\"ni\", \"ku\", \"tu\", \"m\", \"mw\", \"wa\", \"u\", \"i\", \"li\", \"ya\", \"ki\", \"vi\", \"zi\", \"ku\", \"pa\", \"mu\", \"ji\"]\n",
    "\n",
    "\n",
    "RELATIVE = [\"ye\", \"o\", \"yo\", \"lo\", \"cho\", \"vyo\", \"zo\", \"ko\", \"po\", \"mo\"]\n",
    "\n",
    "OBJ_REL = []\n",
    "\n",
    "INDICITIVE_PAST = []\n",
    "\n",
    "for rel in RELATIVE:\n",
    "    for obj in POS_OBJECT:\n",
    "        OBJ_REL.append(rel + obj)\n",
    "        \n",
    "for subj in POS_SUBJECT_NO_A:\n",
    "    for ob in OBJ_REL:\n",
    "        INDICITIVE_PAST.append(subj + \"li\" + ob)\n",
    "\n",
    "INDICITIVE_PAST_NEG = []\n",
    "\n",
    "for neg in NEG_SUBJECT:\n",
    "    for obj in POS_OBJECT:\n",
    "        INDICITIVE_PAST_NEG.append(neg + \"ku\" + obj)\n",
    "        \n",
    "INDICITIVE_PERFECT = []\n",
    "\n",
    "for subj in POS_SUBJECT_NO_A:\n",
    "    for obj in POS_OBJECT:\n",
    "        INDICITIVE_PERFECT.append(subj + \"me\" + obj)\n",
    "        \n",
    "INDICITIVE_PERFECT_NEG = []\n",
    "\n",
    "for neg in NEG_SUBJECT:\n",
    "    for obj in POS_OBJECT:\n",
    "        INDICITIVE_PERFECT_NEG.append(subj + \"ja\" + obj)\n",
    "        \n",
    "INDICITIVE_PRESENT_DEF = []\n",
    "\n",
    "for subj in POS_SUBJECT_NO_A:\n",
    "    for ob in OBJ_REL:\n",
    "        INDICITIVE_PRESENT_DEF.append(subj + \"na\" + ob)\n",
    "\n",
    "INDICITIVE_PRESENT_INDEF = []\n",
    "\n",
    "for subj in POS_SUBJECT_WITH_A:\n",
    "    for obj in POS_OBJECT:\n",
    "        INDICITIVE_PRESENT_INDEF.append(subj + \"a\" + obj)\n",
    "        \n",
    "INDICITIVE_PRESENT_HAB = []\n",
    "\n",
    "for obj in POS_OBJECT:\n",
    "    INDICITIVE_PRESENT_HAB.append(\"hu\" + obj)\n",
    "    \n",
    "INDICITIVE_PRESENT_NEGATIVE = []\n",
    "\n",
    "for neg in NEG_SUBJECT:\n",
    "    for obj in POS_OBJECT:\n",
    "        INDICITIVE_PRESENT_NEGATIVE.append(neg + obj)\n",
    "        \n",
    "INDICITIVE_FUTURE = []\n",
    "\n",
    "for subj in POS_SUBJECT_NO_A:\n",
    "    for ob in OBJ_REL:\n",
    "        INDICITIVE_FUTURE.append(subj + \"taka\" + ob)\n",
    "        \n",
    "INDICITIVE_FUTURE_NEGATIVE = []\n",
    "\n",
    "for neg in NEG_SUBJECT:\n",
    "    for obj in POS_OBJECT:\n",
    "        INDICITIVE_FUTURE_NEGATIVE.append(neg + \"ta\" + obj)\n",
    "        INDICITIVE_FUTURE_NEGATIVE.append(neg + \"to\" + obj)\n",
    "        \n",
    "\n",
    "INDICITIVE = INDICITIVE_PAST + INDICITIVE_PAST_NEG + INDICITIVE_PERFECT + INDICITIVE_PERFECT_NEG + INDICITIVE_PRESENT_DEF\n",
    "\n",
    "INDICITIVE = INDICITIVE + INDICITIVE_PRESENT_INDEF + INDICITIVE_PRESENT_HAB + INDICITIVE_PRESENT_NEGATIVE + INDICITIVE_FUTURE + INDICITIVE_FUTURE_NEGATIVE\n",
    "\n",
    "TENSELESS_RELATIVE = []\n",
    "\n",
    "TENSELESS_REL = []\n",
    "\n",
    "for subj in POS_SUBJECT_NO_A:\n",
    "    for obj in POS_OBJECT:\n",
    "        TENSELESS_REL.append(subj + obj)\n",
    "        \n",
    "TENSELESS_REL_NEG = []\n",
    "\n",
    "for subj in POS_SUBJECT_NO_A:\n",
    "    for ob in OBJ_REL:\n",
    "        TENSELESS_REL_NEG.append(subj + \"si\" + ob)\n",
    "        \n",
    "TENSELESS_RELATIVE = TENSELESS_REL + TENSELESS_REL_NEG\n",
    "\n",
    "CONTEXTUAL = []\n",
    "\n",
    "SITUATIONAL = []\n",
    "\n",
    "for subj in POS_SUBJECT_NO_A:\n",
    "    for obj in POS_OBJECT:\n",
    "        SITUATIONAL.append(subj + \"ki\" + obj)\n",
    "    \n",
    "CONSECUTIVE = []\n",
    "\n",
    "for subj in POS_SUBJECT_NO_A:\n",
    "    for obj in POS_OBJECT:\n",
    "        CONSECUTIVE.append(subj + \"ka\" + obj)\n",
    "    \n",
    "CONTEXTUAL = SITUATIONAL + CONSECUTIVE\n",
    "\n",
    "IMPERATIVE = []\n",
    "\n",
    "SUBJUNCTIVE = []\n",
    "\n",
    "for obj in POS_OBJECT:\n",
    "    IMPERATIVE.append((obj, \"e\"))\n",
    "    IMPERATIVE.append((obj, \"eni\"))\n",
    "    IMPERATIVE.append((\"ka\" + obj, \"e\"))\n",
    "    IMPERATIVE.append((\"ka\" + obj, \"eni\"))\n",
    "\n",
    "for subj in POS_SUBJECT_NO_A:\n",
    "    for obj in POS_OBJECT:\n",
    "        SUBJUNCTIVE.append((subj + obj, \"e\"))\n",
    "        SUBJUNCTIVE.append((subj + \"si\" + obj, \"e\"))\n",
    "        SUBJUNCTIVE.append((subj + \"ka\" + obj, \"e\"))\n",
    "        \n",
    "IMPER_SUBJ = IMPERATIVE + SUBJUNCTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "modular-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list for noun and adjective pairs where adjectives take on the prefix of the noun\n",
    "\n",
    "NOUN_PREFIXES = {1: (\"mw\", \"m\"), 2: (\"wa\", \"w\"), 3: (\"mw\", \"m\"), 4: (\"mi\", \"my\"), 5: (\"ji\", \"j\"), 6: (\"ma\"), \n",
    "                7: (\"ki\", \"ch\"), 8: (\"vi\", \"vy\"), 9: ( \"ny\", \"n\"), 10: (\"ny\", \"n\"), 11: (\"uw\", \"w\", \"u\"), \n",
    "                    14: (\"uw\", \"w\", \"u\"), 15: (\"ku\", \"kw\")}\n",
    "\n",
    "ADJECTIVE_PREFIXES = {1: (\"mwe\", \"mwi\", \"m\"), 2: (\"wa\", \"we\"), 3: (\"mwe\", \"mwi\", \"m\"),  4: (\"mi\", \"mye\"), \n",
    "                      5: (\"ji\", \"je\"), 6: (\"ma\", \"me\"), 7: (\"ki\", \"che\"), 8: (\"vi\", \"vye\"), \n",
    "                      9: (\"ny\", \"nye\", \"mb\", \"nd\", \"ng\", \"nj\", \"mv\", \"mb\", \"nz\"),   \n",
    "                      10: (\"nye\", \"ny\", \"mb\", \"nd\", \"ng\", \"nj\", \"mv\", \"mb\", \"nz\"), 11: (\"mwi\", \"mwe\", \"m\"), \n",
    "                      14: (\"mwi\", \"mwe\", \"m\"), 15: (\"kwi\", \"kwe\", \"ku\")}\n",
    "\n",
    "SHARED_PREFIX = {}\n",
    "\n",
    "for i in range(1, 16):\n",
    "    if i == 12 or i == 13:\n",
    "        continue\n",
    "    SHARED_PREFIX[i] = []\n",
    "    for n_p in NOUN_PREFIXES[i]:\n",
    "        for a_p in ADJECTIVE_PREFIXES[i]:\n",
    "            SHARED_PREFIX[i].append((n_p, a_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "south-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually tagged most frequent words left over\n",
    "\n",
    "MANUAL = {}\n",
    "\n",
    "MANUAL[\"katika\"] = \"PREP\"\n",
    "MANUAL[\"kuwa\"] = \"VERB\"\n",
    "MANUAL[\"kama\"] = \"PREP\"\n",
    "MANUAL[\"yake\"] = \"PREP\"\n",
    "MANUAL[\"wake\"] = \"PREP\"\n",
    "MANUAL[\"baada\"] = \"PREP\"\n",
    "MANUAL[\"kutoka\"] = \"PREP\"\n",
    "MANUAL[\"wakati\"] = \"ADV\"\n",
    "MANUAL[\"nchini\"] = \"NOUN\"\n",
    "MANUAL[\"sasa\"] = \"ADV\"\n",
    "MANUAL[\"pia\"] = \"ADV\"\n",
    "MANUAL[\"au\"] = \"PREP\"\n",
    "MANUAL[\"kuhusu\"] = \"PREP\"\n",
    "MANUAL[\"serikali\"] = \"NOUN\"\n",
    "MANUAL[\"kila\"] = \"ADJ\"\n",
    "MANUAL[\"eneo\"] = \"NOUN\"\n",
    "MANUAL[\"moja\"] = \"NOUN\"\n",
    "MANUAL[\"sababu\"] = \"NOUN\"\n",
    "MANUAL[\"kazi\"] = \"NOUN\"\n",
    "MANUAL[\"yao\"] = \"PREP\"\n",
    "MANUAL[\"wao\"] = \"PREP\"\n",
    "MANUAL[\"kati\"] = \"PREP\"\n",
    "MANUAL[\"mara\"] = \"NOUN\"\n",
    "MANUAL[\"kufanya\"] = \"VERB\"\n",
    "MANUAL[\"hadi\"] = \"PREP\"\n",
    "MANUAL[\"kupitia\"] = \"PREP\"\n",
    "MANUAL[\"kisiasa\"] = \"ADJ\"\n",
    "MANUAL[\"nafasi\"] = \"NOUN\"\n",
    "MANUAL[\"nchi\"] = \"NOUN\"\n",
    "MANUAL[\"akasema\"] = \"VERB\"\n",
    "MANUAL[\"viongozi\"] = \"NOUN\"\n",
    "MANUAL[\"wengi\"] = \"ADJ\"\n",
    "MANUAL[\"uchaguzi\"] = \"NOUN\"\n",
    "MANUAL[\"kubwa\"] = \"ADJ\"\n",
    "MANUAL[\"mbalimbali\"] = \"ADJ\"\n",
    "MANUAL[\"dhidi\"] = \"ADV\"\n",
    "MANUAL[\"kabla\"] = \"ADV\"\n",
    "MANUAL[\"kaunti\"] = \"NOUN\"\n",
    "MANUAL[\"bila\"] = \"ADV\"\n",
    "MANUAL[\"tu\"] = \"ADV\"\n",
    "MANUAL[\"mtu\"] = \"NOUN\"\n",
    "MANUAL[\"shule\"] = \"NOUN\"\n",
    "MANUAL[\"wengine\"] = \"ADJ\"\n",
    "MANUAL[\"kutoa\"] = \"NOUN\"\n",
    "MANUAL[\"maeneo\"] = \"NOUN\"\n",
    "MANUAL[\"fedha\"] = \"NOUN\"\n",
    "MANUAL[\"zao\"] = \"PREP\"\n",
    "MANUAL[\"watoto\"] = \"NOUN\"\n",
    "MANUAL[\"njia\"] = \"NOUN\"\n",
    "MANUAL[\"wananchi\"] = \"NOUN\"\n",
    "MANUAL[\"huduma\"] = \"NOUN\"\n",
    "MANUAL[\"jamii\"] = \"NOUN\"\n",
    "MANUAL[\"taarifa\"] = \"NOUN\"\n",
    "MANUAL[\"dawa\"] = \"NOUN\"\n",
    "MANUAL[\"mkono\"] = \"NOUN\"\n",
    "MANUAL[\"juu\"] = \"PREP\"\n",
    "MANUAL[\"siasa\"] = \"NOUN\"\n",
    "MANUAL[\"chanjo\"] = \"NOUN\"\n",
    "MANUAL[\"kiongozi\"] = \"NOUN\"\n",
    "MANUAL[\"sheria\"] = \"NOUN\"\n",
    "MANUAL[\"habari\"] = \"NOUN\"\n",
    "MANUAL[\"mbali\"] = \"ADV\"\n",
    "MANUAL[\"virusi\"] = \"NOUN\"\n",
    "MANUAL[\"wetu\"] = \"PREP\"\n",
    "MANUAL[\"polisi\"] = \"NOUN\"\n",
    "MANUAL[\"tayari\"] = \"ADV\"\n",
    "MANUAL[\"mbili\"] = \"NOUN\"\n",
    "MANUAL[\"timu\"] = \"NOUN\"\n",
    "MANUAL[\"pesa\"] = \"NOUN\"\n",
    "MANUAL[\"chini\"] = \"ADV\"\n",
    "MANUAL[\"anasema\"] = \"VERB\"\n",
    "MANUAL[\"shughuli\"] = \"NOUN\"\n",
    "MANUAL[\"mchezo\"] = \"NOUN\"\n",
    "MANUAL[\"zake\"] = \"PREP\"\n",
    "MANUAL[\"bado\"] = \"ADV\"\n",
    "MANUAL[\"mechi\"] = \"NOUN\"\n",
    "MANUAL[\"nyingine\"] = \"ADJ\"\n",
    "MANUAL[\"leo\"] = \"ADV\"\n",
    "MANUAL[\"afya\"] = \"NOUN\"\n",
    "MANUAL[\"asilimia\"] = \"NOUN\"\n",
    "MANUAL[\"wiki\"] = \"NOUN\"\n",
    "MANUAL[\"kipindi\"] = \"NOUN\"\n",
    "MANUAL[\"wanafunzi\"] = \"NOUN\"\n",
    "MANUAL[\"ujenzi\"] = \"NOUN\"\n",
    "MANUAL[\"alama\"] = \"NOUN\"\n",
    "MANUAL[\"taifa\"] = \"NOUN\"\n",
    "MANUAL[\"kesi\"] = \"NOUN\"\n",
    "MANUAL[\"wanasiasa\"] = \"NOUN\"\n",
    "MANUAL[\"maafisa\"] = \"NOUN\"\n",
    "MANUAL[\"pili\"] = \"ADJ\"\n",
    "MANUAL[\"lugha\"] = \"NOUN\"\n",
    "MANUAL[\"nyumbani\"] = \"NOUN\"\n",
    "MANUAL[\"maendeleo\"] = \"NOUN\"\n",
    "MANUAL[\"urais\"] = \"NOUN\"\n",
    "MANUAL[\"kampuni\"] = \"NOUN\"\n",
    "MANUAL[\"mambo\"] = \"NOUN\"\n",
    "MANUAL[\"pekee\"] = \"ADJ\"\n",
    "MANUAL[\"tangu\"] = \"ADV\"\n",
    "MANUAL[\"jinsi\"] = \"NOUN\"\n",
    "MANUAL[\"wangu\"] = \"PREP\"\n",
    "MANUAL[\"ripoti\"] = \"NOUN\"\n",
    "MANUAL[\"mpya\"] = \"ADJ\"\n",
    "MANUAL[\"nyingi\"] = \"ADJ\"\n",
    "MANUAL[\"uwezo\"] = \"NOUN\"\n",
    "MANUAL[\"yetu\"] = \"PREP\"\n",
    "MANUAL[\"vizuri\"] = \"ADV\"\n",
    "MANUAL[\"wakazi\"] = \"NOUN\"\n",
    "MANUAL[\"hasa\"] = \"ADJ\"\n",
    "MANUAL[\"elimu\"] = \"NOUN\"\n",
    "MANUAL[\"mtoto\"] = \"NOUN\"\n",
    "MANUAL[\"masuala\"] = \"NOUN\"\n",
    "MANUAL[\"biashara\"] = \"NOUN\"\n",
    "MANUAL[\"msimu\"] = \"NOUN\"\n",
    "MANUAL[\"tena\"] = \"ADV\"\n",
    "MANUAL[\"tatu\"] = \"NOUN\"\n",
    "MANUAL[\"wawili\"] = \"NOUN\"\n",
    "MANUAL[\"yangu\"] = \"PREP\"\n",
    "MANUAL[\"jijini\"] = \"NOUN\"\n",
    "MANUAL[\"jambo\"] = \"NOUN\"\n",
    "MANUAL[\"bidhaa\"] = \"NOUN\"\n",
    "MANUAL[\"mswada\"] = \"NOUN\"\n",
    "MANUAL[\"nyumba\"] = \"NOUN\"\n",
    "MANUAL[\"kawaida\"] = \"ADV\"\n",
    "MANUAL[\"duniani\"] = \"NOUN\"\n",
    "MANUAL[\"changamoto\"] = \"NOUN\"\n",
    "MANUAL[\"ardhi\"] = \"NOUN\"\n",
    "MANUAL[\"saa\"] = \"NOUN\"\n",
    "MANUAL[\"mradi\"] = \"NOUN\"\n",
    "MANUAL[\"uhusiano\"] = \"NOUN\"\n",
    "MANUAL[\"kuu\"] = \"ADJ\"\n",
    "MANUAL[\"ushindi\"] = \"NOUN\"\n",
    "MANUAL[\"kadhaa\"] = \"ADJ\"\n",
    "MANUAL[\"fursa\"] = \"NOUN\"\n",
    "MANUAL[\"karibu\"] = \"ADV\"\n",
    "MANUAL[\"mfano\"] = \"NOUN\"\n",
    "MANUAL[\"vifaa\"] = \"NOUN\"\n",
    "MANUAL[\"kuanza\"] = \"VERB\"\n",
    "MANUAL[\"mahakama\"] = \"NOUN\"\n",
    "MANUAL[\"hatari\"] = \"NOUN\"\n",
    "MANUAL[\"familia\"] = \"NOUN\"\n",
    "MANUAL[\"kiasi\"] = \"NOUN\"\n",
    "MANUAL[\"mwezi\"] = \"NOUN\"\n",
    "MANUAL[\"matokeo\"] = \"NOUN\"\n",
    "MANUAL[\"dakika\"] = \"NOUN\"\n",
    "MANUAL[\"matumizi\"] = \"NOUN\"\n",
    "MANUAL[\"wanasema\"] = \"VERB\"\n",
    "MANUAL[\"sawa\"] = \"ADJ\"\n",
    "MANUAL[\"usiku\"] = \"NOUN\"\n",
    "MANUAL[\"kitaifa\"] = \"ADV\"\n",
    "MANUAL[\"kufikia\"] = \"VERB\"\n",
    "MANUAL[\"kwake\"] = \"PREP\"\n",
    "MANUAL[\"rasmi\"] = \"ADJ\"\n",
    "MANUAL[\"mdogo\"] = \"NOUN\"\n",
    "MANUAL[\"lengo\"] = \"NOUN\"\n",
    "MANUAL[\"usalama\"] = \"NOUN\"\n",
    "MANUAL[\"mpenzi\"] = \"NOUN\"\n",
    "MANUAL[\"rais\"] = \"NOUN\"\n",
    "MANUAL[\"idadi\"] = \"NOUN\"\n",
    "MANUAL[\"ugonjwa\"] = \"NOUN\"\n",
    "MANUAL[\"mazingira\"] = \"NOUN\"\n",
    "MANUAL[\"asema\"] = \"VERB\"\n",
    "MANUAL[\"mdogo\"] = \"NOUN\"\n",
    "MANUAL[\"lengo\"] = \"NOUN\"\n",
    "MANUAL[\"usalama\"] = \"NOUN\"\n",
    "MANUAL[\"mpenzi\"] = \"NOUN\"\n",
    "MANUAL[\"rais\"] = \"NOUN\"\n",
    "MANUAL[\"idadi\"] = \"NOUN\"\n",
    "MANUAL[\"ugonjwa\"] = \"NOUN\"\n",
    "MANUAL[\"mazingira\"] = \"NOUN\"\n",
    "MANUAL[\"asema\"] = \"VERB\"\n",
    "MANUAL[\"wazazi\"] = \"NOUN\"\n",
    "MANUAL[\"teknolojia\"] = \"NOUN\"\n",
    "MANUAL[\"sekta\"] = \"NOUN\"\n",
    "MANUAL[\"nini\"] = \"PREP\"\n",
    "MANUAL[\"janga\"] = \"NOUN\"\n",
    "MANUAL[\"maambukizi\"] = \"NOUN\"\n",
    "MANUAL[\"ndoa\"] = \"NOUN\"\n",
    "MANUAL[\"kuendelea\"] = \"VERB\"\n",
    "MANUAL[\"badala\"] = \"ADV\"\n",
    "MANUAL[\"mfumo\"] = \"NOUN\"\n",
    "MANUAL[\"uongozi\"] = \"NOUN\"\n",
    "MANUAL[\"bilioni\"] = \"NOUN\"\n",
    "MANUAL[\"sita\"] = \"NOUN\"\n",
    "MANUAL[\"moyo\"] = \"NOUN\"\n",
    "MANUAL[\"madai\"] = \"NOUN\"\n",
    "MANUAL[\"raia\"] = \"NOUN\"\n",
    "MANUAL[\"mama\"] = \"NOUN\"\n",
    "MANUAL[\"mke\"] = \"NOUN\"\n",
    "MANUAL[\"jina\"] = \"NOUN\"\n",
    "MANUAL[\"maji\"] = \"NOUN\"\n",
    "MANUAL[\"kushiriki\"] = \"VERB\"\n",
    "MANUAL[\"klabu\"] = \"NOUN\"\n",
    "MANUAL[\"soka\"] = \"NOUN\"\n",
    "MANUAL[\"mengi\"] = \"ADJ\"\n",
    "MANUAL[\"lake\"] = \"PREP\"\n",
    "MANUAL[\"kwenda\"] = \"VERB\"\n",
    "MANUAL[\"uchumi\"] = \"NOUN\"\n",
    "MANUAL[\"nguvu\"] = \"NOUN\"\n",
    "MANUAL[\"yeye\"] = \"PREP\"\n",
    "MANUAL[\"mashtaka\"] = \"NOUN\"\n",
    "MANUAL[\"msingi\"] = \"NOUN\"\n",
    "MANUAL[\"kikosi\"] = \"NOUN\"\n",
    "MANUAL[\"kuweka\"] = \"VERB\"\n",
    "MANUAL[\"wala\"] = \"PREP\"\n",
    "MANUAL[\"juhudi\"] = \"NOUN\"\n",
    "MANUAL[\"dunia\"] = \"NOUN\"\n",
    "MANUAL[\"mtandao\"] = \"NOUN\"\n",
    "MANUAL[\"chake\"] = \"PREP\"\n",
    "MANUAL[\"nyuma\"] = \"NOUN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "average-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that iterates through every two words in a given set\n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = itertools.tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dedicated-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pandas dataframe that holds tokens, with punctuation and numbers included\n",
    "#\n",
    "tokens = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "df = pd.DataFrame()\n",
    "df['Word'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "painted-austria",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#takes in the text corpus and tags data, returning a dictionary formatted as 'token': 'tag'\n",
    "def posTagger(text):\n",
    "    tokens = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    starting_size = len(tokens)\n",
    "    print(\"Starting Text Corpus Size:\", starting_size, \"tokens\")\n",
    "    \n",
    "    tags = {}\n",
    "\n",
    "    #tags proper nouns\n",
    "    for x, y in pairwise(tokens):\n",
    "        if x not in string.punctuation and y[0].isupper():\n",
    "            if y not in tags:\n",
    "                tags[y] = \"PROPN\"\n",
    "    \n",
    "    #decreases size of token list for future loops by removing tagged data after tagging some data\n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "    proper = starting_size - len(tokens)\n",
    "    print(\"Total Proper Nouns Marked:\", len(tags))\n",
    "            \n",
    "    #all punctuation other than periods taking out with regular language when setting 'tokens' variable    \n",
    "    tags[\".\"] = \"PUNCT\"\n",
    "    punct = len(tokens)\n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "    punct -= len(tokens)\n",
    "    print(\"Total Periods Marked:\", punct)\n",
    "\n",
    "    #numbers represented by digits, not with words\n",
    "    for token in tokens:\n",
    "        if token.isdigit():\n",
    "            if token not in tags:\n",
    "                tags[token] = \"NUMBER\"\n",
    "                \n",
    "    numbers = len(tokens)\n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "    numbers -= len(tokens)\n",
    "    print(\"Total Numbers Marked:\", numbers)\n",
    "\n",
    "    #prepositions and pronouns\n",
    "    for token in tokens:\n",
    "        if token.endswith(\"enye\"):\n",
    "            if token not in tags:\n",
    "                tags[token] = \"PREP\"\n",
    "            continue\n",
    "        for prep in PREPOSITIONS:\n",
    "            if token == prep:\n",
    "                if token not in tags:\n",
    "                    tags[token] = \"PREP\"\n",
    "                break\n",
    "    \n",
    "    preps = len(tokens)\n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "    preps -= len(tokens)\n",
    "    print(\"Total Prepositions/Pronouns Marked:\", preps)\n",
    "\n",
    "    #determiners\n",
    "    for token in tokens:\n",
    "        if token.endswith(\"ote\") or token.endswith(\"pi\") or token.endswith(\"enyewe\"):\n",
    "            if token not in tags:\n",
    "                tags[token] = \"DETER\"\n",
    "            continue\n",
    "        for d in DETERMINERS:\n",
    "            if token == d:\n",
    "                if token not in tags:\n",
    "                    tags[token] = \"DETER\"\n",
    "                break\n",
    "            \n",
    "    deter = len(tokens)\n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "    deter -= len(tokens)\n",
    "    print(\"Total Determiners Marked:\", deter)\n",
    "\n",
    "    #relative adjectives\n",
    "    for token in tokens:\n",
    "        for r_a in RELATIVE_ADJECTIVES_PREFIXES:\n",
    "            if token.startswith(r_a):\n",
    "                if token not in tags:\n",
    "                    tags[token] = \"ADJ\"\n",
    "                break\n",
    "            \n",
    "    rel_adj = len(tokens)\n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "    rel_adj -= len(tokens)\n",
    "    print(\"Total Relative Adjectives Marked:\", rel_adj)\n",
    "\n",
    "    #verbs with known suffixes\n",
    "    for token in tokens:\n",
    "        for k in KNOWN_VERB_SUFFIX:\n",
    "            if token.endswith(k):\n",
    "                if token not in tags:\n",
    "                    tags[token] = \"VERB\"\n",
    "                break\n",
    "\n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "    \n",
    "    #irrealis verbs and other known verbs\n",
    "    for token in tokens:\n",
    "        for i in IRREALIS_VERB:\n",
    "            if i in token:\n",
    "                if token not in tags:\n",
    "                    tags[token] = \"VERB\"\n",
    "                break\n",
    "            \n",
    "            \n",
    "        for v in VERB_EQUALITY:\n",
    "            if token == v:\n",
    "                if token not in tags:\n",
    "                    tags[token] = \"VERB\"\n",
    "                break  \n",
    "            \n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "\n",
    "    #indicitive verbs\n",
    "    for token in tokens:            \n",
    "        for i in INDICITIVE:\n",
    "            if token.startswith(i):\n",
    "                if token not in tags:\n",
    "                    tags[token] = \"VERB\"\n",
    "                break\n",
    "            \n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "            \n",
    "    #relative and contextual verbs\n",
    "    for token in tokens:\n",
    "        temp = TENSELESS_RELATIVE + CONTEXTUAL\n",
    "        for i in temp:\n",
    "            if token.startswith(i) and not (token == i):\n",
    "                if token not in tags:\n",
    "                    tags[token] = \"VERB\"\n",
    "                break\n",
    "            \n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "        \n",
    "    #imperative and subjunctive verbs\n",
    "    for token in tokens:\n",
    "        for i in IMPER_SUBJ:\n",
    "            if token.startswith(i[0]) and token.endswith(i[1]) and len(token) > 4:\n",
    "                if token not in tags:\n",
    "                    tags[token] = \"VERB\"\n",
    "                break\n",
    "\n",
    "\n",
    "    verbs = len(tokens)\n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "    verbs -= len(tokens)\n",
    "    print(\"Total Verbs Marked:\", verbs)\n",
    "\n",
    "    #known concord pairs, where adjectives take on nouns\n",
    "    marked = False\n",
    "    for x, y in pairwise(tokens):\n",
    "        if marked == True:\n",
    "            marked = False\n",
    "            continue\n",
    "        for i in range(1, 16):\n",
    "            if i == 12 or i == 13:\n",
    "                continue\n",
    "            if x.startswith(SHARED_PREFIX[i][0]) and y.startswith(SHARED_PREFIX[i][1]):\n",
    "                if x not in tags:\n",
    "                    tags[x] = \"NOUN\"\n",
    "                if x not in tags:\n",
    "                    tags[x] = \"ADJ\"\n",
    "                marked = True\n",
    "            break\n",
    "        \n",
    "    concord = len(tokens)\n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "    concord -= len(tokens)\n",
    "    print(\"Total Concord Noun-Adjectives Marked:\", concord)\n",
    "\n",
    "    #manually tagged data\n",
    "    for token in tokens:\n",
    "        if token in MANUAL:\n",
    "            tags[token] = MANUAL[token]\n",
    "        \n",
    "    manual = len(tokens)\n",
    "    tokens = [t for t in tokens if t not in tags.keys()]\n",
    "    manual -= len(tokens)\n",
    "    print(\"Total Words Manually Marked:\", manual)\n",
    "\n",
    "    print(\"Total Tokens Tagged:\", starting_size - len(tokens), \"tokens\")\n",
    "    return tags\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "japanese-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store tags in pandas dataframe as a new column called \"PoS\"\n",
    "def createTraining():\n",
    "    pos = []\n",
    "    i = 0\n",
    "    for token in tokens:\n",
    "        pos.append(\"NaN\")\n",
    "        if token in TAGS:\n",
    "            pos[i] = TAGS[token]\n",
    "        i += 1\n",
    "    df['PoS'] = pos\n",
    "    with open('sw_training.txt', 'w+') as f:\n",
    "        for word in TAGS:\n",
    "            f.write(\"{word}\\{tag} \".format(word = word, tag = TAGS[word]))\n",
    "    print(\"sw_training.txt file created!\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "nuclear-organizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Text Corpus Size: 543774 tokens\n",
      "Total Proper Nouns Marked: 11127\n",
      "Total Periods Marked: 17699\n",
      "Total Numbers Marked: 7073\n",
      "Total Prepositions/Pronouns Marked: 96208\n",
      "Total Determiners Marked: 18683\n",
      "Total Relative Adjectives Marked: 9266\n",
      "Total Verbs Marked: 4962\n",
      "Total Concord Noun-Adjectives Marked: 180\n",
      "Total Words Manually Marked: 85888\n",
      "Total Tokens Tagged: 434834 tokens\n"
     ]
    }
   ],
   "source": [
    "TAGS = posTagger(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "right-engineer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sw_training.txt file created!\n"
     ]
    }
   ],
   "source": [
    "createTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "pending-escape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Swahili words tagged: 31043\n",
      "Lexical Diversity: 0.08383997763776863\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique Swahili words tagged:\", len(TAGS))\n",
    "print(\"Lexical Diversity:\", len(set(tokens))/len(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
