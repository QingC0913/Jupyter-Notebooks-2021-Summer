{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'>Evaluating Regression models</font>\n",
    "\n",
    " Evaluating models is an essential step in doing Machine Learning for two reasons:\n",
    "\n",
    "- So we can evaluate and find the model that performs best.\n",
    "- So we can calculate the expected margin of error in our predictions.\n",
    "\n",
    "We are going to look at various error metrics that judge how well regression models make predictions. \n",
    "\n",
    "Before we begin, let's load the necessary libraries for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(6,6)}) \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still using the boston housing dataset from the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the boston dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split data into training and testing sets, fit the model on the training set and made predictions on the test set. \n",
    "\n",
    "P.S to learn more about the `random_state` parameter, see [this Stack Overflow answer](https://stackoverflow.com/a/28069274)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train_test_split from sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data using a test_size of 0.33 and set random_state to 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the model using the `LinearRegression` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LinearRegression from sklearn.linear_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object of the LinearRegression class and call it model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "\n",
    "# make predictions\n",
    "\n",
    "\n",
    "# see predictions\n",
    "for y, y_pred in list(zip(y_test, predictions))[:5]:\n",
    "    print(\"Real value: {:.3f} Estimated value: {:.5f}\".format(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now look at metrics to evaluate our predictions.\n",
    "\n",
    "This allows us to get an indication of how good or how poor our predictions actually were - without just eyeballing some visualisations :)\n",
    "\n",
    "Model evaluation metrics in `scikit-learn` are available under the module `metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics from sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'>Mean Absolute Error (MAE)</font>\n",
    "\n",
    "The Mean Absolute Error (MAE) is defined as:\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}|y_i -\\hat{y}_i|$$\n",
    "\n",
    "Basically the differences between the real values of the target variable and the predictions in absolute value (so turning negative differences into positive ones).\n",
    "\n",
    "MAE is a robust metric, that means it doesnt change dramatically when there are outliers. The MAE error can be interpreted in the same units of the target variable (so for example, if the target variable is in dollars, the MAE will also be in dollars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y_test, predictions)\n",
    "print(\"The Mean Absolute Error is {:.3f} dollars\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](media/train_test_split.png)\n",
    "![title](media/train_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'>Mean Squared Error (MSE)</font>\n",
    "\n",
    "The Mean Squared Error (MSE) is defined as:\n",
    "\n",
    "$$\\frac{1}{n}\\sum_{i=1}^{n}(y_i -\\hat{y}_i)^2$$\n",
    "\n",
    "Similar to the MAE, but using the square of the difference between the true target and the prediction. \n",
    "\n",
    "MSE gives more weight to larger errors than MAE (is not robust to outliers). For example, let's imagine we are predicting housing prices using the Boston dataset and we have the following observations:\n",
    "\n",
    "```\n",
    "observation1: MEDV: 10  MEDV_pred: 15    MSE: (10-15)²=25\n",
    "observation2: MEDV: 1000 MEDV_pred: 1010 MSE: (1000-1010)²=100 \n",
    "```\n",
    "\n",
    "By using MSE we are giving more weight to the error on observation2 than on observation1, even though a 5000`$` error on a 15,000`$` house is a much worse error than a 10,000`$` error on a 1,000,000`$` house.\n",
    "\n",
    "The MSE is measured in squared units (squared dollars?) which is hard to understand, so there is another metric called Root Mean Squared Error (RMSE) that is just the root of the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = metrics.mean_squared_error(y_test, predictions)\n",
    "print(\"The Mean Squared Error is {:.3f} dollars²\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'>Root Mean Squared Error (RMSE)</font>\n",
    "\n",
    "Root Mean Squared Error (RMSE) is just the root of the MSE, and it is measured in the same units as the target variable.\n",
    "\n",
    "$$\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i -\\hat{y}_i)^2}$$\n",
    "\n",
    "Similarly to MSE, RMSE is more sensitive than MAE to variations in errors. Here we can see an example of this.\n",
    "\n",
    "![mae_vs_mse](media/mse_vs_mae.png)\n",
    "\n",
    "In this example we can see that on the 3 cases MAE remains the same, while RMSE can be equal to MAE or much larger depending on the error distribution.\n",
    "\n",
    "So in those cases where we care about making big mistakes we can use RMSE. For example, if we are predicting student grades, we might not care that much about individual errors (predicting a 10 when the truth was a 2 is not a big deal), but about the overall performance (and we could use MAE). If we are predicting house values to purchase them, predicting a million dollars when the actual house price is 20,000$ is an error we cant afford!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn doesnt have rmse, but it's easy to create the metric \"manually\"\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, predictions))\n",
    "print(\"The Root Mean Squared Error is {:.3f} dollars\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'>R2 (Coefficient of Determination)</font>\n",
    "\n",
    "The Coefficient of Determination (R2, pronounced *R-squared*) measures the portion of the variance that can be explained by the model.\n",
    "\n",
    "R2 ranges from (-1 to 1) (a model explaining all the variance would have an $r^2$ of 1).\n",
    "\n",
    "[There are many ways to measure $r^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination) , but one of the simplest ones is simply the squared [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)  between the true target and the prediction, squared.\n",
    "\n",
    "$$r^2=\\frac{\\sum_{i=1}^n(\\hat y_i-\\bar y)^2}{\\sum_{i=1}^n(y_i-\\bar y)^2}.$$\n",
    "\n",
    "#### <font color='#eb3483'>Adjusted R2</font>\n",
    "\n",
    "There is an updated version of R2 called **Adjusted R-squared** that takes into consideration model complexity (so it penalizes a complex model versus a simple one).\n",
    "\n",
    "$$1 - \\frac{(1 - R^2)(n-1)}{(n-k-1)}$$\n",
    "\n",
    "where `n` is the number of observations and `k` is the number of model coefficients.\n",
    "\n",
    "<!--Contrary to `MAE` and `MSE` the higher the R2 the better.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r2 = metrics.r2_score(y_test, predictions)\n",
    "model_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr = np.corrcoef(y_test, predictions)\n",
    "model_r2_ = pearson_corr**2\n",
    "model_r2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate easily the adjusted r² value by hand, to account for model complexity. This value depends on the number of coefficients and imposes a penalty for additional coefficients\n",
    "<!--(number of coefficients). \n",
    ", the regularization lesson goes further into the issue of model complexity.-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X_test)\n",
    "k = len(model.coef_)\n",
    "r2 = model_r2\n",
    "\n",
    "adjusted_r2_model = 1 - ((1-r2)*(n-1)/(n-k-1))\n",
    "adjusted_r2_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the adjusted r² of the model is smaller than the original r²."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate our models. First we create a dictionary to store the results, and a function to help us evaluate models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"Function to evaluate models, you could add more metrics here!\"\"\"\n",
    "    return {\n",
    "        \"mae\": metrics.mean_absolute_error(y_true, y_pred),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "# model_ols = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first train and predict without splitting data\n",
    "model_ols.fit(X=boston['data'], y=boston['target'])\n",
    "model_ols_preds = model_ols.predict(boston['data'])\n",
    "\n",
    "# store results\n",
    "RESULTS[\"ols\"] = evaluate_model(\n",
    "    boston[\"target\"],\n",
    "    model_ols_preds,\n",
    ")\n",
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and predict using just training dataset\n",
    "model_ols.fit(X=X_train, y=y_train)\n",
    "model_ols_train_preds = model_ols.predict(X_train)\n",
    "\n",
    "RESULTS[\"ols_train\"] = evaluate_model(\n",
    "    y_train,\n",
    "    model_ols_train_preds\n",
    ")\n",
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the test dataset\n",
    "model_ols_test_preds = model_ols.predict(X_test)\n",
    "RESULTS[\"ols_test\"] = evaluate_model(\n",
    "    y_test,\n",
    "    model_ols_test_preds\n",
    ")\n",
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we get we get worse results on the test set than on the training set.\n",
    "\n",
    "We could just stop here and say *\"Our model MAE on the test dataset is 3.56...\"*, and we could think everything is ok with this because that is the error on unseen data.\n",
    "\n",
    "However, this would be a big mistake, why? \n",
    "\n",
    "Remember, we have used a specific random state `random_state=13`, what would happen if we use another seed, for example, `random_state=42`?\n",
    "\n",
    "**Let's run everything again with a diffefent random seed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = {}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     boston[\"data\"],   # X\n",
    "     boston[\"target\"], # y\n",
    "     test_size=0.33, \n",
    "     random_state=42\n",
    ")\n",
    "\n",
    "model_ols = LinearRegression()\n",
    "\n",
    "model_ols.fit(X=X_train, y=y_train)\n",
    "model_ols_train_preds = model_ols.predict(X_train)\n",
    "model_ols_test_preds = model_ols.predict(X_test)\n",
    "\n",
    "\n",
    "RESULTS[\"ols_train2\"] = evaluate_model(\n",
    "    y_train,\n",
    "    model_ols_train_preds\n",
    ")\n",
    "\n",
    "RESULTS[\"ols_test2\"] = evaluate_model(\n",
    "    y_test,\n",
    "    model_ols_test_preds\n",
    ")\n",
    "\n",
    "pd.DataFrame(RESULTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The test MAE is lower than the train one!!, WHAAAT!!??** How can that even be possible???\n",
    "\n",
    "Very simple, it just turns out the split generated by using the seed number 42 puts on the test dataset observations that are easier to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Cross-validation </font>\n",
    "\n",
    "One way to avoid evaluating on one single lucky split is by doing **Cross Validation**. When we do cross validation we simply split the data into **N** partitions, for each partion, we train the model with the remaining N-1 partitions and evaluate on that one. That way we get N evaluation errors trained and evaluated on different data so we don't rely on a single split. We finally make predictions on the test set to get an unbiased view of model performance.\n",
    "\n",
    "For example, a 5 fold cross validation would look like this:\n",
    "\n",
    "![title](media/cross_validation.png)\n",
    "\n",
    "<!--![title](media/grid_search_cross_validation.png)-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` has a function `cross_val_score` that evaluates a model doing cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `cross_val_score`, we need a model, the independent and dependent variables (X and y). We also have to define an evaluation metric (`scoring` argument) and the number of splits (`cv` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose any of the scorers defined on `sklearn.metrics.SCORERS` or we can create our own scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import SCORERS\n",
    "SCORERS.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we want to perform cross validation using the mean absolute error (MAE), we can use the scoring `neg_mean_squared_error`, that is the name of the mean absolute error on the `SCORERS` dictionary above.\n",
    "\n",
    "**Note**: In scikit-learn, the \"bad\" scores (i.e. errors) are returned as negative numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cross_validation = cross_val_score(\n",
    "    estimator=model, \n",
    "    X=boston[\"data\"],\n",
    "    y=boston[\"target\"],\n",
    "    scoring=\"neg_mean_absolute_error\", \n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cross_val_score` returns the evaluation on the test set for each one of the splits (in this case, 5 splits). Normal number of splits are between 3 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally calculate the average partition error to get a better estimation of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use abs to get positive MAE values\n",
    "model_mae = abs(results_cross_validation.mean())\n",
    "print(f\"the average MAE for 5 split cross validation is : {model_mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define our own cross validation evaluation metric, which is simply a function that expects to receive as arguments the trained estimator, X and y and return the error.\n",
    "\n",
    "For example, if we want to use rmse as a scoring for cross validation, we can create our own scoring function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_cross_val(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return np.sqrt(metrics.mean_squared_error(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cross_validation_rmse = cross_val_score(\n",
    "    estimator=model, \n",
    "    X=boston[\"data\"],\n",
    "    y=boston[\"target\"],\n",
    "    scoring=rmse_cross_val, \n",
    "    cv=5, \n",
    ")\n",
    "\n",
    "mean_rmse_cv = results_cross_validation_rmse.mean()\n",
    "print(f\"the average RMSE for 5 split cross validation is : {mean_rmse_cv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A bit on hyperparameters.** The function `cross_val_score` has the argument `cv`. We can adjust its value to improve the model score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'>BONUS: The function cross_validate</font>\n",
    "\n",
    "If we want to get more information about each split, we can use the function `cross_validate` that returns more information besides the test error. It also accepts multiple scoring functions instead of just one. Think of `cross_val_score` as the simplified version of `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring_functions = {\"mae\": \"neg_mean_absolute_error\", \"rmse\": rmse_cross_val}\n",
    "\n",
    "scores = cross_validate(\n",
    "    model,                # estimator\n",
    "    boston[\"data\"],            # X\n",
    "    boston[\"target\"],          # y\n",
    "    scoring=scoring_functions, # a single scorer or a dict with multiple scoring functions\n",
    "    cv=10,                      # number of partitions\n",
    "    return_train_score=True    # return the training error, not only the test error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get results for each one of the partitions:\n",
    "- fit time, how long it takes to train the model\n",
    "- score time, how long it takes to predict\n",
    "- test and train scores for each one of the scoring functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the averages for all the partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
