{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> Evaluating Classification Models </font>\n",
    "In this module, we'll be exploring classification models more in-depth, namely how can we evaluate our models to see how they're performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(6,6)}) #Set our seaborn aesthetics (we're going to customize our figure size)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Preparing our Data </font>\n",
    "\n",
    "We'll keep using our breast cancer dataset from the last module. We'll quickly get it into a format that will facilitate exploring module evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset \n",
    "cancer = ...\n",
    "\n",
    "# create a new dataframe called \"cancer_df\" using the data we loaded.\n",
    "# the columns are from the \"feature_names\" key\n",
    "cancer_df = pd.DataFrame(...)\n",
    "\n",
    "# add a new column, \"target\", from the \"target\" key\n",
    "cancer_df[\"target\"] = ...\n",
    "\n",
    "cancer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer[\"target_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this dataset encodes the 0 as malignant and 1 as benign."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are going to replace the target 0 with 1, so the positive class is malignant. We do this because usually a positive test means detection of cancer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the 0s in the \"target\" column with 1s\n",
    "cancer_df[\"target\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the percentage of cases for each class (positive and negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the counts of the \"target\" column. Hint: use value_counts(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there are 62.7% negative cases (benign) and 37.3% positives (malignant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Training a Model </font>\n",
    "\n",
    "We will start by training a simple Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the LogisticRegression class from scikit-learn\n",
    "from sklearn.model_selection import ...\n",
    "\n",
    "# import train_test_split from from scikit-learn\n",
    "from sklearn.linear_model import ...\n",
    "\n",
    "# import metrics from from scikit-learn\n",
    "from sklearn import ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and testing\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "# use 30% of the dataset as the testing set, and set random_state to 42\n",
    "X_train, X_test, y_train, y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit the model, and generate predicted labels and prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the logistic regression\n",
    "\n",
    "model = ...\n",
    "\n",
    "model.fit(...)\n",
    "\n",
    "# get the predictions\n",
    "predictions = ...\n",
    "true_classes = y_test\n",
    "prediction_probabilities = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an auxiliary function that returns a list of true target values and their predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> Binary Classification Concepts </font>\n",
    "\n",
    "In Binary classification we have *negative cases* (class 0, on the Breast Cancer dataset would be the benign samples) and *positive cases* (class 1, malignant samples).\n",
    "\n",
    "- Positive Cases: Cases of class 1 (malignant)\n",
    "- Negative Cases: Cases of class 0 (benign)\n",
    "\n",
    "These 2 classes combined with the predictions bring us to 4 possible combinations:\n",
    "\n",
    "- True positives (TP), would be the samples that are malignant and are correctly classified as malignant. \n",
    "- False positives (FP), would be the benign samples that are incorrectly classified as malignant.\n",
    "- True Negatives (TN), would be the benign samples that are correctly classified as benign.\n",
    "- False Negatives (FN), would be the malignant samples that are incorrectly classified as benign.\n",
    "\n",
    "![title](media/classification_errors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> **Confusion Matrix** </font>\n",
    "\n",
    "We can use a confusion matrix to easily compare how a classifier has classified each one of the classes.\n",
    "\n",
    "![title](media/confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import confusion_matrix from sklearn.metrics \n",
    "\n",
    "# create a confusion matrix using the true classes and predictions\n",
    "confusion_matrix(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> Classification Metrics </font>\n",
    "\n",
    "<font color='#eb3483'> **Accuracy** </font>\n",
    "\n",
    "Accuracy is a general measure of the model's performance. It simply measures the percentage of cases correctly classified.\n",
    "\n",
    "$$Accuracy=\\frac{\\text{Number of correctly classified observations}}{\\text{Total Number of observations}}= \\frac{TP+TN}{TP+TN+FP+FN}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn has a function that calculates the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the accuracy using the true classes and predictions\n",
    "metrics.accuracy_score(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#eb3483'> **Precision** </font>\n",
    "\n",
    "Precision measures the model's hability to correctly classify as positives the positive cases.\n",
    "\n",
    "$$Precision=\\frac{\\text{Number of positive cases correctly classified}}{\\text{Number of cases classified as positive}}= \\frac{TP}{TP+FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the precision using the true classes and predictions\n",
    "metrics.precision_score(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/precision_accuracy.png\" style=\"width:30em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#eb3483'> **Recall (True Positive Rate, TPR)** </font>\n",
    " \n",
    "Recall gives us an idea of the model's ability to find (detect) all positive cases.\n",
    "\n",
    "$$Recall=\\frac{\\text{Number of positive cases correctly classified}}{\\text{Number of positive classes}}= \\frac{TP}{TP+FN}$$\n",
    "\n",
    "\n",
    "![title](media/precision_recall.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the recall using the true classes and predictions\n",
    "metrics.recall_score(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#eb3483'> **F1 Score** </font>\n",
    "\n",
    "F1 score is a weighted measure between recall (that tries to classify as many cases as possible as positive cases) and precision (that tries to classify as positive only real positive cases and limit false positives).\n",
    "\n",
    "F1 Score is defined as the harmonic mean between precision and recall.\n",
    "\n",
    "$$F1=2*\\frac{1}{\\frac{1}{precision}+\\frac{1}{recall}}=2*\\frac{precision*recall}{precision+recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 score is available in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the F1 score using the true classes and predictions\n",
    "metrics.f1_score(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#eb3483'> How does a model classify? </font>\n",
    "\n",
    "An algorithm like logistic regression predicts by measuring distances to a \"decission boundary\" that are then transformed into class probabilities. \n",
    "\n",
    "But at the end of the day we need to know which class to assign to a new observation, and not just the predicted probabilities. Classifiers do that by defining a *threshold* and then assigning a negative class to all those cases with probabilities lower than the threshold and positive those above it.\n",
    "\n",
    "![title](media/threshold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually use the method `model.predict` for predicting a target variable. However, some methods also have a method `predict_proba` that predicts the probabilities that the model consider that the observation has to belong to each one of the classes.\n",
    "\n",
    "For the binary classification case, `predict_proba` will predict for each observation the probabilities of it being a negative and the probabilities of being a positive case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first 5 prediction probabilities\n",
    "prediction_probabilities[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"true_class\":true_classes,\n",
    "                   \"pred_class\": predictions,\n",
    "                   \"probabilities_0\":model.predict_proba(X_test)[:,0],\n",
    "                    \"probabilities_1\":model.predict_proba(X_test)[:,1],\n",
    "                  })\n",
    "\n",
    "df[\"sum_probas\"] = df.probabilities_0 + df.probabilities_1\n",
    "\n",
    "df.sum_probas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for each row, the sum of the probabilities is 1 (which makes sense since its the whole sample space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the scikit-learn classifier choose a threshold? Because it has no additional information, it just sets the threshold to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"probabilities_1>0.5 & pred_class==0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(\"probabilities_0>0.5 & pred_class==1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Area Under the Curve (ROC-AUC) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiving Operating Characteristic [(ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) is a curve used to evaluate how Recall (TPR) and FPR change based on the threshold. It shows how the model balances the opposing efects of classifying correctly all positive cases without having many false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to see how the predictions change based on our threshold level - let's create a function that takes the class probabilities and a desired threshold and returns the predicted class based off that threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilities_to_classes(prediction_probabilities, threshold=0.5):\n",
    "    predictions = np.zeros([len(prediction_probabilities), ])\n",
    "    predictions[prediction_probabilities[:,1]>=threshold] = 1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first 10 prediction probabilities\n",
    "prediction_probabilities[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see easily convert those probabilities to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_to_classes(prediction_probabilities, threshold=0.5)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the threshold is closer to 0, more observations will be a positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_to_classes(prediction_probabilities, threshold=0.00001)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if the desired threshold is closer to 1, less observations will be predicted as a positive (only those where the model is really really sure about them being a positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_to_classes(prediction_probabilities, threshold=0.99999)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The area under the ROC curve (that is the part of the chart below the curve) is called **Area under the Curve (ROC-AUC or simply AUC)** and is one of the most common metrics on classification problems. It ranges from 0.5 (a random classifier) to 1 (the perfect classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(true_classes, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `sklearn.roc_curve` to generate the ranges for false positive range and true positive range automatically. And we can make the plot so we compare it to a random classifier. *(no need to learn everything, is matplotlib messy code)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve(true_classes, predictions, prediction_probabilities):\n",
    "    fpr, tpr, _ = metrics.roc_curve(true_classes, prediction_probabilities[:,1])\n",
    "    roc_auc = metrics.roc_auc_score(true_classes, predictions)\n",
    "\n",
    "    sns.mpl.pyplot.fill_between(fpr, tpr, step='post', alpha=0.2,color='b')\n",
    "    sns.lineplot(x=fpr, y=tpr, linestyle='--', label='ROC Curve(area = %0.2f)' % roc_auc)\n",
    "    sns.lineplot(x=[0,1], y= [0,1], linestyle='--', label = 'Random Classifier')\n",
    "    \n",
    "    sns.mpl.pyplot.xlabel('FPR')\n",
    "    sns.mpl.pyplot.ylabel('TPR (recall)')\n",
    "    sns.mpl.pyplot.title('ROC Curve')\n",
    "\n",
    "roc_curve(true_classes, predictions, prediction_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#eb3483'> Precission-Recall Curve </font>\n",
    "\n",
    "The precision-Recall curve gives us an idea of how the precision and recall vary depending on the threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use scikit-learn `metrics.precision_recall_curve` to calculate the steps for the curve directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_curve(true_classes, prediction_probabilities):\n",
    "    precision_, recall_, _ = metrics.precision_recall_curve(\n",
    "        true_classes, prediction_probabilities[:,1])\n",
    "\n",
    "    sns.lineplot(recall_,precision_, drawstyle='steps-pre', ci=None)\n",
    "    sns.mpl.pyplot.fill_between(recall_, precision_, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "    sns.mpl.pyplot.xlabel('Recall')\n",
    "    sns.mpl.pyplot.ylabel('Precision')\n",
    "    sns.mpl.pyplot.title('Precision-Recall Curve');\n",
    "\n",
    "\n",
    "precision_recall_curve(true_classes, prediction_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now the obvious question, **why do we need so many metrics, can't we just use accuracy?** Check out the advanced exercises for an answer :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "widgets": {
   "state": {
    "faecd23e356c4a88b55f22e7d579b0f3": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
