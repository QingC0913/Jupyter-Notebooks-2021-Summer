{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> Evaluating Clustering Methods </font>\n",
    "Once we have clusters, a natural question is to ask how good they are. In this notebook we'll walk through a few quick ways to check the performance of our clusters (these are great for comparing different clustering methods). In general we'll look at two cases:\n",
    "\n",
    "1. External Methods: Ways to evaluate our clusters when we have a ground truth label (i.e. what the clusters should be)\n",
    "2. Internal Methods: Ways to evaluate our clusters when we have no 'right' answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#eb3483'> Data Loading </font>\n",
    "\n",
    "We are going to use the [20Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/), which is a collection of approximately 20,000 newsgroup (newsgroups were basically internet forums in the old internet) documents, partitioned (nearly) evenly across 20 different newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups_vectorized, fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "news_20 = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\n",
      "Subject: PB questions...\n",
      "Organization: Purdue University Engineering Computer Network\n",
      "Distribution: usa\n",
      "Lines: 36\n",
      "\n",
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n",
      "---------------------------------------------------------------------------\n",
      "\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\n",
      "Nietzsche\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_20.data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly download the text in vectorized form (TF-IDF) with `scikit-learn`, which returns a sparse matrix. \n",
    "\n",
    "tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups_vectorized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the observations labels with `target_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  7, 10, ..., 14, 12, 11])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 20 natural clusters (20 different news rooms). Let's quickly try to make our own 20 clusters using K means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MiniBatchKMeans(n_clusters=20)\n",
    "estimator.fit(news)\n",
    "pred_labels = estimator.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  9, 16, ..., 12, 14, 16], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'> External Evaluation Measures </font>\n",
    "\n",
    "We call external evaluation measures those that rely on \"true\" cluster labels known beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_classes = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "                            homogeneity_completeness_v_measure, \n",
    "                            adjusted_rand_score, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function [homogeneity_completeness_v_measure](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure) returns a tuple with homogeneity, completeness and v-measure. What do those terms actually mean? \n",
    "\n",
    "- **Homogeneity**: How alike are all the data-points in a cluster (i.e. do they all have the same true cluster label). In general we want clusters that are as homogenous as possible.\n",
    "- **Completeness**: For a given true label (i.e. sports news) completeness indicates how much of the data points with that label are in the same cluster. For example, if we just put all our data in the same cluster, we'd have a completeness of 1.\n",
    "- **V-Measure**: This is a fancy score that looks at the \"mutual information\" between the cluster label and the true label (i.e. if we see the cluster label, how much do we then know about the true label). \n",
    "\n",
    "All of these values are normalized between 0 and 1, with 1 being the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Model Performance:\n",
      "      Homogeneity: 0.04688327357652485\n",
      "      Completeness: 0.08694884839520874\n",
      "      V-Measure: 0.06091880762881924\n"
     ]
    }
   ],
   "source": [
    "homogeneity, completeness, v_measure =  homogeneity_completeness_v_measure(real_classes, pred_labels)\n",
    "\n",
    "print(f\"\"\"Cluster Model Performance:\n",
    "      Homogeneity: {homogeneity}\n",
    "      Completeness: {completeness}\n",
    "      V-Measure: {v_measure}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see these clusters have higher completeness than homogeneity (that means, the clusters are less homogeneous than the class distribution in the clusters). We also have a low V-Measure meaning these clusters arent great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For datasets smaller than 1000 observations or for a cluster number greater than 10, it is recommended to evaluate using the Adjusted Rand Index (ARI), available as the function [adjusted_rand_score](http://scikit-learn.org/stable/modules/clustering.html#adjusted-rand-index). ARI looks at how similar our cluster assignments are to the true label (you can dig into the details later), for now just understand that larger positive values are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016464237308071956"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_rand_score(real_classes, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ARI has a range of (-1, 1), so these clusters aren't great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use external evaluation metrics for crossvalidation the same way we could do with any other classification/regression problem.\n",
    "\n",
    "Metrics defined on `cross_val_score` include:\n",
    "\n",
    "- `adjusted_rand_score`  \n",
    "- `completeness_score` \t \n",
    "- `homogeneity_score` \t \n",
    "- `v_measure_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we can evaluate the performance of the MiniBatchKMeans in terms of the Adjusted Rand Index with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(X=news, y=real_classes, \n",
    "                             estimator=MiniBatchKMeans(), \n",
    "                             scoring=\"adjusted_rand_score\", cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02071193587831173"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#eb3483'>  Internal Evaluation metrics </font>\n",
    "\n",
    "Internal measures are used when the true classes aren't known before hand (which is usually the case). To evaluate clusters when we don't have a ground truth label we want to try to evaluate how 'similar' our data is within-in clusters and how 'different' data is between clusters. Luckily for us, sklearn has some built-in methods that give us a view on that trade-off. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabaz_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These internal measures are used in `sklearn` with 2 arguments, the training dataset and the cluster labels.\n",
    "\n",
    "First we have the Silhouette Coefficient [(silhouette_score)](http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient), goes from -1 to 1. the higher the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11035924060560391"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silhouette_score(news, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, our clustering sucks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Calinski-harabaz score [(calinski-harabaz-score)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score) is another internal evaluation metric. It gives us a measure of dispersion within a cluster and of the separation between clusters. It is faster to compute than the Silhouette Coefficient. The higher the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.49986943724563"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calinski_harabaz_score(news.todense(), pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
